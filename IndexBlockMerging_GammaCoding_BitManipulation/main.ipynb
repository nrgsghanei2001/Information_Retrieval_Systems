{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the Block's documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenize(list_of_contexts):\n",
    "    all_tokens = []         # a list to save all of tokens of all documents of a block\n",
    "    for i, doc in enumerate(list_of_contexts):\n",
    "        lower_doc = doc.lower()              # make all of contexts lower case\n",
    "        list_of_contexts[i] = lower_doc          \n",
    "        tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_contexts[i])   # tokenize the text with regex\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords and other useless tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def token_filtering(all_tokens):\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        new_tokens = []\n",
    "        for token in doc:                           # delete all of stopwords and single character tokens except numbers from token list\n",
    "            if (len(token) < 2 and token.isalpha()) or (token in stop_words):          \n",
    "                continue\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        all_tokens[i] = new_tokens\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Term-TermID map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_to_termID(term_termID_map, all_tokens, count):\n",
    "    for doc in all_tokens:\n",
    "        for token in doc:\n",
    "            if token not in list(term_termID_map.keys()):\n",
    "                term_termID_map[token] = count             # add non repeated tokens to term_termID map with a uniqe term ID\n",
    "                count += 1\n",
    "\n",
    "    return term_termID_map, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma encoding and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gamma_encode_bitwise(docIDs):  # function for gamma encoding the docIDs list\n",
    "    gaps = [docIDs [0]] + [docIDs [i] - docIDs [i-1] for i in range(1, len(docIDs))]\n",
    "    gamma = ''            # empty string for the gamma encoded string\n",
    "    for x in gaps:      \n",
    "        msb = math.floor(math.log2(x))     # find the highest power of 2 that x contains, which is equivalent to finding the position of the most significant bit of x\n",
    "        unary = '0' * msb + '1'            # generate a unary code for the msb position, which is a string of msb zeros followed by a one\n",
    "        binary = bin(x)[3:]         # generate a binary code for x, which is the binary representation of x without the leading one  \n",
    "        gamma += unary + binary           # concatenate the unary code and the binary code to get the gamma code for x\n",
    "\n",
    "    return gamma      # return the gamma encoded string\n",
    "\n",
    "\n",
    "def decode_gamma(gamma):         # function for gamma decoding the docIDs list\n",
    "    decoded = []       # an empty list for the decoded docIDs\n",
    "    current = 0      # the current docID\n",
    "    # split the gamma encoded string into gamma codes for each gap\n",
    "    # use a loop to read and count the ones until reaching the first zero\n",
    "    i = 0\n",
    "    while i < len(gamma):\n",
    "        count = 0\n",
    "        while gamma [i] == '0':\n",
    "            count += 1\n",
    "            i += 1\n",
    "    \n",
    "        binary = '1' + gamma [i+1:i+1+count]        # read the next count bits and prepend a one to them\n",
    "        gap = int(binary, 2)            # convert the binary code to a decimal number\n",
    "        current += gap           # add the gap to the current docID to get the next docID\n",
    "        decoded.append(current)    # append the next docID to the decoded list\n",
    "        i += 1 + count            # update the index to skip the zero and the binary code\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the key of a given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val, my_dict):     # return the key for a given value in dictionary  \n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted Index's Data Structue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.posting_list = \"\"\n",
    "        self.is_word = False\n",
    "        self.term_freq = 0\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    # function to insert new terms to the trie tree\n",
    "    def insert(self, word, docID):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, add it as a new child node\n",
    "            if char not in current_node.children:\n",
    "                current_node.children[char] = TrieNode()\n",
    "            current_node = current_node.children[char]\n",
    "        # Mark the end of the word\n",
    "        current_node.is_word = True\n",
    "        current_node.term_freq += 1\n",
    "        # add document ID to posting list of the added term\n",
    "        docIDs = decode_gamma(current_node.posting_list)\n",
    "        if docID not in docIDs:\n",
    "            docIDs.append(docID)\n",
    "            sorted(docIDs)\n",
    "            gamma = gamma_encode_bitwise(docIDs)\n",
    "            current_node.posting_list = gamma\n",
    "\n",
    "    # function to check if a term is present in the tree or not\n",
    "    def search(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, the word is not in the trie\n",
    "            if char not in current_node.children:\n",
    "                return False\n",
    "            current_node = current_node.children[char]\n",
    "        return current_node.is_word\n",
    "\n",
    "    # function to return the term's node when it is present in the tree\n",
    "    def get(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            current_node = current_node.children[char]\n",
    "        # Return the last node itself\n",
    "        return current_node\n",
    "\n",
    "    # A function to print all the terms in the trie\n",
    "    def print_words(self, node=None, prefix=\"\"):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        # If the node is a word, print the prefix\n",
    "        if node.is_word:\n",
    "            print(prefix, node.term_freq , decode_gamma(node.posting_list))\n",
    "        # Loop through each child node\n",
    "        for char, child in node.children.items():\n",
    "            # Recursively call the function with the child node and the updated prefix\n",
    "            self.print_words(child, prefix + char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the address and names of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the documents are: \n",
      "A Festival of Books - A Murder-Suicide - Better To Be Unlucky - Cloning Pets - Crazy Housing Prices - Food Fight Erupted in Prison - Freeway Chase Ends at Newsstand - Gasoline Prices Hit Record High - Happy and Unhappy Renters - Jerry Decided To Buy a Gun - Man Injured at Fast Food Place - Pulling Out Nine Tons of Trash - Rentals at the Oceanside Community - Sara Went Shopping - Trees Are a Threat - "
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# where the folder of all docs is\n",
    "path = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\documents\" \n",
    "os.chdir(path) \n",
    "name_of_docs = []  # name of documents\n",
    "list_of_docs_address = []  # list of address of documents\n",
    "\n",
    "for file in os.listdir():                # save the file passes in a list\n",
    "    file_path = f\"{path}\\{file}\"\n",
    "    list_of_docs_address.append(file_path)\n",
    "    name_of_docs.append(file[:-4])\n",
    "\n",
    "\n",
    "print(\"All of the documents are: \")\n",
    "for name in name_of_docs:\n",
    "    print(name, end=\" - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each block's documents and preprocess them, and build the (term, docID) pairs and sort them for each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_read(doc_per_block):\n",
    "    my_DISC = []                      # a simulation of computer's disc\n",
    "    blocks = []                       # save all blocks\n",
    "    term_termID_map = {}              # map each term with a term ID\n",
    "    i, count = 0, 0\n",
    "    while i < len(list_of_docs_address):   # put each n documents in a block\n",
    "        block = []\n",
    "        for j in range(i, i+doc_per_block):\n",
    "            if j < len(list_of_docs_address):\n",
    "                block.append(list_of_docs_address[j])\n",
    "        blocks.append(block)\n",
    "        i += doc_per_block\n",
    "\n",
    "    for block_number, block in enumerate(blocks):    # do for each block\n",
    "        list_of_contexts = []\n",
    "        termID_docID_pair = []\n",
    "        for doc in block:                            # do for exh document in a block\n",
    "            context = \"\" \n",
    "            with open(doc, 'r') as f:  \n",
    "                context = f.read()\n",
    "                list_of_contexts.append(context)            # read the context of the document and put it in a list\n",
    "        all_tokens = tokenize(list_of_contexts)     # tokenize and preprocess the context of each block\n",
    "        all_tokens = token_filtering(all_tokens)\n",
    "        term_termID_map, count = term_to_termID(term_termID_map, all_tokens, count)  # update term termID map\n",
    "        for j, doc in enumerate(all_tokens):\n",
    "            docID = (block_number * doc_per_block) + j\n",
    "            for token in doc:\n",
    "                termID = term_termID_map[token]\n",
    "                termID_docID_pair.append((termID, docID))       # pair each term with document ID in a touple and put it in a list for later proccessing\n",
    "\n",
    "        termID_docID_pair = sorted(termID_docID_pair)   # sort pairs for each block\n",
    "        my_DISC.append(termID_docID_pair)   # save the result of each block in dics\n",
    "    \n",
    "    return my_DISC, term_termID_map\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "my_disc, term_termID_map = block_read(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the result of each block and build the final inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_blocks(my_disc):\n",
    "    inverted_index = Trie()      # create the inverted index\n",
    "    heap = []                  # a min heap\n",
    "    for block_number, block in enumerate(my_disc):\n",
    "        heap.append((block[0][0], block[0][1], block_number))   # push first element of each block to min heap, insert the block id of them\n",
    "    end = False\n",
    "    while not end:       # till all blocks becoe empty\n",
    "        heap= sorted(heap)       # sort the heap to be a min heap\n",
    "        term = get_key(heap[0][0], term_termID_map)    # pick the smallest element of heap\n",
    "        docID = heap[0][1] + 1\n",
    "        block_number = heap[0][2]\n",
    "        inverted_index.insert(term, docID)        # insert the min element to the inverted index\n",
    "\n",
    "        heap.pop(0)                      # pop the inserted element from heap and its block\n",
    "        my_disc[block_number].pop(0)\n",
    "        if len(my_disc[block_number]) != 0:      # if its block isn't empty push the next minimum element of the block to heap\n",
    "            heap.append((my_disc[block_number][0][0], my_disc[block_number][0][1], block_number))\n",
    "        end = True\n",
    "        for block in my_disc:    # check if all blocks are empty\n",
    "            if len(block)!=0:\n",
    "                end = False\n",
    "                break\n",
    "\n",
    "    return inverted_index    \n",
    "\n",
    "new_dics = my_disc \n",
    "inverted_index = merge_blocks(new_dics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people 10 [1, 2, 8, 9, 13, 15]\n",
      "per 2 [8, 13]\n",
      "percent 9 [1, 5, 6, 8, 9, 13, 15]\n",
      "period 2 [1, 3]\n",
      "perfect 1 [4]\n",
      "permitted 1 [9]\n",
      "persons 1 [9]\n",
      "personal 1 [15]\n",
      "penny 1 [8]\n",
      "popular 4 [1, 6]\n",
      "police 11 [2, 7, 8, 9, 12, 13]\n",
      "portable 1 [12]\n",
      "post 1 [13]\n",
      "park 1 [13]\n",
      "parking 3 [1, 13]\n",
      "parked 1 [7]\n",
      "part 1 [13]\n",
      "party 1 [13]\n",
      "pain 1 [2]\n",
      "paid 4 [5, 14, 15]\n",
      "pay 4 [3, 4, 5, 15]\n",
      "paying 1 [8]\n",
      "paper 2 [5, 6]\n",
      "passing 2 [5, 8]\n",
      "pasadena 1 [14]\n",
      "price 5 [4, 5, 8, 14]\n",
      "prices 7 [1, 5, 8, 15]\n",
      "prison 3 [6]\n",
      "prisoners 3 [6]\n",
      "prince 2 [11]\n",
      "private 1 [15]\n",
      "president 1 [4]\n",
      "pretty 2 [8, 15]\n",
      "produce 3 [4, 13]\n",
      "profit 1 [5]\n",
      "provide 1 [6]\n",
      "proven 1 [13]\n",
      "proprietor 1 [7]\n",
      "proposal 1 [13]\n",
      "property 2 [15]\n",
      "problem 2 [11, 13]\n",
      "problems 4 [13, 15]\n",
      "probably 1 [11]\n",
      "proceeds 1 [12]\n",
      "pros 1 [13]\n",
      "practice 1 [10]\n",
      "practiced 1 [9]\n",
      "place 3 [2, 5, 10]\n",
      "play 1 [3]\n",
      "playing 3 [9]\n",
      "played 1 [9]\n",
      "player 2 [9]\n",
      "playground 1 [15]\n",
      "plates 1 [6]\n",
      "plastic 1 [12]\n",
      "plants 1 [15]\n",
      "planners 1 [15]\n",
      "plowed 1 [7]\n",
      "plot 1 [10]\n",
      "plus 1 [14]\n",
      "piano 2 [3, 9]\n",
      "pianist 1 [9]\n",
      "picked 3 [3, 9]\n",
      "pizza 1 [10]\n",
      "pistol 1 [10]\n",
      "pile 1 [13]\n",
      "pine 1 [15]\n",
      "phone 1 [3]\n",
      "phoenix 1 [4]\n",
      "puppies 1 [4]\n",
      "put 2 [7, 15]\n",
      "pursuing 1 [7]\n",
      "pumping 1 [8]\n",
      "pulp 1 [10]\n",
      "joke 1 [1]\n",
      "joy 1 [3]\n",
      "job 2 [9, 12]\n",
      "john 3 [11, 14]\n",
      "johnson 1 [11]\n",
      "jumped 1 [3]\n",
      "jail 1 [7]\n",
      "japanese 1 [10]\n",
      "jerry 8 [10]\n",
      "one 25 [1, 2, 3, 5, 7, 9, 10, 11, 12, 13, 15]\n",
      "ones 3 [9, 10]\n",
      "onto 1 [7]\n",
      "outdoor 1 [1]\n",
      "outdoors 1 [1]\n",
      "outside 1 [7]\n",
      "ought 1 [15]\n",
      "occurs 1 [1]\n",
      "occurred 3 [1, 2, 12]\n",
      "octane 1 [8]\n",
      "oceanside 1 [13]\n",
      "old 11 [2, 3, 7, 10, 11, 12, 14, 15]\n",
      "owner 4 [3, 5, 13]\n",
      "owners 1 [13]\n",
      "owns 1 [14]\n",
      "overpaying 1 [5]\n",
      "overpaid 1 [5]\n",
      "offer 1 [5]\n",
      "offers 1 [5]\n",
      "official 1 [13]\n",
      "officials 6 [5, 6, 13, 15]\n",
      "officer 2 [7]\n",
      "officers 4 [6, 7]\n",
      "offset 1 [13]\n",
      "often 1 [8]\n",
      "opportunity 1 [5]\n",
      "operation 1 [15]\n",
      "order 1 [6]\n",
      "ordering 2 [7, 11]\n",
      "ok 1 [8]\n",
      "oil 1 [9]\n",
      "others 1 [13]\n",
      "los 5 [1, 7]\n",
      "lose 2 [2, 5]\n",
      "love 1 [1]\n",
      "loved 2 [10]\n",
      "lot 3 [2, 8, 13]\n",
      "lottery 3 [3]\n",
      "local 2 [2, 5]\n",
      "long 3 [5, 8]\n",
      "look 1 [15]\n",
      "looking 3 [5, 15]\n",
      "lowest 1 [8]\n",
      "lozano 1 [13]\n",
      "loud 1 [13]\n",
      "loan 1 [15]\n",
      "loans 1 [15]\n",
      "libraries 1 [1]\n",
      "lived 3 [2, 10, 14]\n",
      "lives 2 [4, 6]\n",
      "living 1 [15]\n",
      "like 10 [2, 5, 6, 9, 10]\n",
      "liked 1 [10]\n",
      "light 2 [2, 12]\n",
      "life 4 [2, 3, 9, 12]\n",
      "listen 2 [9, 13]\n",
      "listening 1 [2]\n",
      "little 4 [3, 5, 15]\n",
      "liquid 1 [4]\n",
      "lips 1 [5]\n",
      "line 2 [8]\n",
      "lines 2 [8]\n",
      "lined 1 [12]\n",
      "limp 1 [10]\n",
      "last 4 [2, 5]\n",
      "lake 1 [3]\n",
      "late 1 [13]\n",
      "later 3 [4, 5, 11]\n",
      "latest 1 [9]\n",
      "landlord 1 [9]\n",
      "landfill 1 [12]\n",
      "lane 1 [11]\n",
      "lap 1 [11]\n",
      "large 1 [11]\n",
      "lawn 1 [13]\n",
      "less 2 [2]\n",
      "left 2 [3, 9]\n",
      "leftovers 1 [9]\n",
      "lee 3 [4]\n",
      "least 1 [5]\n",
      "leading 1 [7]\n",
      "legal 1 [6]\n",
      "let 1 [11]\n",
      "letting 1 [11]\n",
      "luxury 1 [6]\n",
      "lucky 4 [7, 9, 15]\n",
      "angeles 5 [1, 7]\n",
      "angelenos 1 [1]\n",
      "annual 2 [1, 9]\n",
      "anniversary 1 [2]\n",
      "answer 1 [1]\n",
      "anyone 1 [5]\n",
      "anything 1 [12]\n",
      "another 4 [9, 13]\n",
      "antennas 1 [12]\n",
      "available 1 [1]\n",
      "avoid 1 [7]\n",
      "avoided 1 [1]\n",
      "avenue 2 [3, 7]\n",
      "average 1 [8]\n",
      "april 1 [1]\n",
      "apparent 1 [2]\n",
      "appliances 1 [2]\n",
      "apply 1 [15]\n",
      "appears 1 [3]\n",
      "approved 1 [6]\n",
      "apartment 1 [10]\n",
      "apartments 1 [9]\n",
      "attendance 1 [1]\n",
      "attract 1 [7]\n",
      "attitudes 1 [9]\n",
      "authors 3 [1]\n",
      "auto 1 [12]\n",
      "autograph 1 [1]\n",
      "audience 1 [1]\n",
      "american 1 [1]\n",
      "among 1 [1]\n",
      "amount 1 [4]\n",
      "amounts 1 [13]\n",
      "amputated 1 [2]\n",
      "ambulance 1 [7]\n",
      "ammunition 1 [10]\n",
      "ago 7 [1, 2, 6, 8, 13]\n",
      "agency 1 [2]\n",
      "aged 1 [9]\n",
      "although 2 [1, 12]\n",
      "altadena 1 [2]\n",
      "already 5 [1, 2, 4, 13]\n",
      "allen 3 [2]\n",
      "allotments 1 [6]\n",
      "allow 1 [13]\n",
      "allowing 1 [13]\n",
      "almost 5 [2, 8, 12, 13]\n",
      "always 6 [2, 5, 9]\n",
      "alone 1 [2]\n",
      "along 1 [12]\n",
      "also 5 [3, 7, 9, 10, 13]\n",
      "alan 1 [12]\n",
      "according 1 [2]\n",
      "accidentally 2 [10, 11]\n",
      "actually 1 [4]\n",
      "acres 1 [15]\n",
      "afghan 1 [3]\n",
      "arizona 1 [4]\n",
      "around 2 [5, 8]\n",
      "arcadia 1 [8]\n",
      "arrived 1 [14]\n",
      "area 3 [15]\n",
      "asking 1 [5]\n",
      "asked 2 [8, 15]\n",
      "assisted 1 [12]\n",
      "administration 1 [6]\n",
      "away 3 [8, 14]\n",
      "aid 1 [11]\n",
      "able 1 [11]\n",
      "abuse 1 [13]\n",
      "reads 1 [1]\n",
      "reading 1 [1]\n",
      "ready 1 [4]\n",
      "realtor 2 [5]\n",
      "reason 3 [8, 9]\n",
      "rent 5 [9, 13]\n",
      "rents 1 [1]\n",
      "renters 4 [9, 13]\n",
      "renting 1 [9]\n",
      "rental 1 [13]\n",
      "rentals 3 [13]\n",
      "resident 2 [14, 15]\n",
      "residents 7 [1, 8, 13, 15]\n",
      "rest 1 [12]\n",
      "restored 1 [6]\n",
      "restaurant 5 [10, 11]\n",
      "resumed 1 [7]\n",
      "resulted 1 [13]\n",
      "respond 2 [13]\n",
      "response 1 [13]\n",
      "retired 1 [2]\n",
      "retirees 1 [12]\n",
      "return 1 [6]\n",
      "repairs 1 [3]\n",
      "receives 1 [4]\n",
      "recent 1 [15]\n",
      "reconsider 1 [13]\n",
      "remaining 1 [4]\n",
      "remarked 1 [5]\n",
      "remove 1 [15]\n",
      "removed 3 [15]\n",
      "released 1 [6]\n",
      "related 1 [9]\n",
      "reduction 1 [6]\n",
      "reduce 1 [6]\n",
      "reduced 1 [8]\n",
      "revolver 2 [10]\n",
      "refused 1 [11]\n",
      "refill 1 [11]\n",
      "regular 1 [14]\n",
      "raffle 1 [3]\n",
      "rates 1 [5]\n",
      "ration 2 [6]\n",
      "rather 1 [8]\n",
      "rattle 1 [10]\n",
      "razors 1 [6]\n",
      "ran 2 [7]\n",
      "range 1 [15]\n",
      "raises 1 [9]\n",
      "rain 1 [12]\n",
      "raining 1 [10]\n",
      "rainstorm 1 [15]\n",
      "radios 1 [12]\n",
      "raging 1 [15]\n",
      "run 5 [3, 5, 7, 8]\n",
      "ruined 1 [7]\n",
      "right 3 [3, 8, 10]\n",
      "rick 1 [13]\n",
      "road 1 [5]\n",
      "roadside 1 [12]\n",
      "room 1 [6]\n",
      "roof 2 [9]\n",
      "ever 1 [3]\n",
      "every 7 [1, 2, 8, 9, 12, 13]\n",
      "everyone 6 [1, 5, 12, 14]\n",
      "everett 2 [8]\n",
      "even 9 [1, 2, 4, 5, 8, 12, 13]\n",
      "evening 1 [11]\n",
      "event 1 [12]\n",
      "explain 1 [1]\n",
      "experimenting 1 [4]\n",
      "exhibitors 1 [1]\n",
      "except 2 [1, 7]\n",
      "excitedly 1 [3]\n",
      "exchange 1 [6]\n",
      "exact 1 [5]\n",
      "extra 3 [11, 13, 15]\n",
      "estimated 1 [1]\n",
      "estrus 1 [4]\n",
      "ethnic 1 [1]\n",
      "embrace 1 [1]\n",
      "emphysema 1 [2]\n",
      "empty 2 [7]\n",
      "employee 2 [11]\n",
      "eye 1 [2]\n",
      "eyesight 1 [2]\n",
      "eight 4 [2, 4, 8, 12]\n",
      "either 1 [2]\n",
      "ending 1 [2]\n",
      "ended 1 [7]\n",
      "enough 1 [7]\n",
      "english 1 [10]\n",
      "environmental 1 [12]\n",
      "erupted 1 [6]\n",
      "early 1 [7]\n",
      "earring 1 [12]\n",
      "economy 1 [8]\n",
      "elsewhere 1 [8]\n",
      "elevation 1 [15]\n",
      "watch 4 [10, 12]\n",
      "watches 1 [1]\n",
      "watching 1 [5]\n",
      "water 1 [7]\n",
      "wash 1 [11]\n",
      "washes 1 [1]\n",
      "wait 3 [1, 4, 8]\n",
      "waited 1 [8]\n",
      "waiting 1 [11]\n",
      "want 1 [5]\n",
      "warden 1 [6]\n",
      "walked 1 [10]\n",
      "walks 1 [10]\n",
      "way 2 [14, 15]\n",
      "west 2 [1, 7]\n",
      "week 7 [2, 6, 13]\n",
      "weekend 2 [1, 9]\n",
      "weeks 1 [4]\n",
      "weekly 2 [13]\n",
      "well 4 [4, 10, 12, 15]\n",
      "weaned 1 [4]\n",
      "wearing 1 [7]\n",
      "went 8 [6, 7, 10, 14]\n",
      "website 1 [13]\n",
      "would 28 [1, 2, 3, 6, 9, 10, 11, 12, 13]\n",
      "woman 2 [2]\n",
      "world 2 [2, 9]\n",
      "worth 4 [3, 5, 12, 13]\n",
      "work 8 [8, 10, 12, 13, 15]\n",
      "worked 1 [12]\n",
      "worms 1 [13]\n",
      "woke 1 [14]\n",
      "wife 2 [2]\n",
      "widow 1 [15]\n",
      "widowed 1 [2]\n",
      "win 2 [3]\n",
      "winnings 1 [3]\n",
      "windows 1 [6]\n",
      "whose 1 [4]\n",
      "whole 2 [9]\n",
      "whatever 1 [5]\n",
      "whales 2 [12]\n",
      "whether 2 [5, 13]\n",
      "wheel 1 [7]\n",
      "wheels 1 [8]\n",
      "whiskey 1 [7]\n",
      "white 1 [10]\n",
      "wrong 2 [5, 9]\n",
      "tv 4 [1, 3, 12]\n",
      "traffic 3 [1, 8, 15]\n",
      "trade 1 [6]\n",
      "trash 5 [12, 13]\n",
      "tried 2 [6, 7]\n",
      "triple 1 [12]\n",
      "try 1 [15]\n",
      "trying 1 [8]\n",
      "trucks 2 [12, 15]\n",
      "tree 1 [15]\n",
      "trees 5 [15]\n",
      "talk 3 [1, 2]\n",
      "talks 1 [1]\n",
      "talking 2 [2, 3]\n",
      "talked 1 [2]\n",
      "tax 1 [14]\n",
      "taxes 1 [3]\n",
      "table 1 [5]\n",
      "take 1 [12]\n",
      "taken 1 [7]\n",
      "tank 1 [8]\n",
      "th 1 [2]\n",
      "thing 2 [3]\n",
      "things 5 [2, 5, 7]\n",
      "think 2 [13, 15]\n",
      "thinks 1 [13]\n",
      "thick 1 [15]\n",
      "three 7 [3, 7, 10, 12, 13, 14]\n",
      "threw 2 [6, 7]\n",
      "though 1 [8]\n",
      "thought 2 [5, 12]\n",
      "thousands 1 [15]\n",
      "theater 1 [10]\n",
      "thelma 1 [15]\n",
      "tons 4 [2, 12, 15]\n",
      "together 1 [2]\n",
      "tough 1 [3]\n",
      "told 2 [5, 9]\n",
      "tower 3 [6]\n",
      "town 3 [15]\n",
      "toward 1 [15]\n",
      "toilet 1 [6]\n",
      "today 2 [8]\n",
      "took 1 [10]\n",
      "toss 1 [13]\n",
      "totally 1 [13]\n",
      "toyola 1 [14]\n",
      "toaster 1 [14]\n",
      "toppled 1 [15]\n",
      "twice 3 [2, 4, 15]\n",
      "twin 3 [4]\n",
      "two 21 [2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15]\n",
      "twenty 1 [5]\n",
      "tim 2 [5]\n",
      "time 5 [2, 4, 10, 12]\n",
      "times 6 [3, 8, 10]\n",
      "tired 2 [5, 9]\n",
      "tires 1 [12]\n",
      "tell 3 [2, 8, 10]\n",
      "telling 1 [5]\n",
      "teen 1 [3]\n",
      "teenage 1 [3]\n",
      "teenager 1 [3]\n",
      "tune 1 [9]\n",
      "tuner 1 [3]\n",
      "turned 1 [7]\n",
      "turning 1 [7]\n",
      "video 3 [10]\n",
      "videos 2 [1, 10]\n",
      "vittorio 2 [2]\n",
      "vittorios 1 [2]\n",
      "victoria 1 [2]\n",
      "vicky 1 [2]\n",
      "visiting 1 [5]\n",
      "visitors 1 [13]\n",
      "variable 2 [3]\n",
      "variables 2 [3]\n",
      "value 1 [12]\n",
      "veterinarian 1 [4]\n",
      "venice 2 [5]\n",
      "vendor 1 [7]\n",
      "vehicle 1 [7]\n",
      "vehicles 1 [8]\n",
      "volunteers 2 [12]\n",
      "go 10 [2, 5, 8, 9, 10, 11]\n",
      "goes 4 [1, 4, 5, 9]\n",
      "good 2 [1, 15]\n",
      "gone 1 [2]\n",
      "got 7 [2, 3, 7, 14, 15]\n",
      "going 4 [3, 15]\n",
      "goal 1 [4]\n",
      "government 2 [5, 15]\n",
      "golf 2 [12]\n",
      "guides 1 [1]\n",
      "gun 4 [10]\n",
      "gunshots 1 [2]\n",
      "guess 2 [3]\n",
      "guessing 1 [3]\n",
      "guessed 1 [3]\n",
      "guarantee 1 [3]\n",
      "guards 3 [6]\n",
      "guy 2 [9, 10]\n",
      "give 1 [5]\n",
      "given 2 [1, 11]\n",
      "gift 1 [11]\n",
      "great 3 [1, 5, 12]\n",
      "greater 1 [3]\n",
      "grow 1 [4]\n",
      "growing 1 [4]\n",
      "grocery 1 [8]\n",
      "groups 2 [12]\n",
      "grabbed 1 [6]\n",
      "grand 1 [7]\n",
      "get 8 [1, 2, 5, 8, 9, 13, 15]\n",
      "getting 1 [3]\n",
      "gets 1 [13]\n",
      "geyser 1 [7]\n",
      "gear 1 [12]\n",
      "generating 1 [13]\n",
      "game 1 [3]\n",
      "gas 6 [8]\n",
      "gasoline 1 [8]\n",
      "gallon 4 [8, 14]\n",
      "gangster 2 [10]\n",
      "gangsters 2 [10]\n",
      "gave 1 [11]\n",
      "garbage 2 [12]\n",
      "gloves 1 [12]\n",
      "movie 4 [1, 10]\n",
      "movies 2 [1, 10]\n",
      "move 1 [5]\n",
      "moved 3 [9]\n",
      "month 2 [2, 3]\n",
      "months 1 [8]\n",
      "money 5 [3, 8, 10, 13, 15]\n",
      "monica 1 [5]\n",
      "morning 1 [10]\n",
      "mountain 1 [15]\n",
      "material 1 [1]\n",
      "magazine 1 [7]\n",
      "magazines 2 [1, 6]\n",
      "man 7 [2, 7, 8, 9, 11]\n",
      "many 3 [1, 9]\n",
      "managed 1 [7]\n",
      "manager 4 [8, 10, 11, 13]\n",
      "married 2 [2, 14]\n",
      "market 1 [5]\n",
      "made 2 [5, 14]\n",
      "main 1 [6]\n",
      "maintenance 1 [13]\n",
      "making 2 [8, 13]\n",
      "make 3 [15]\n",
      "major 1 [15]\n",
      "massive 1 [15]\n",
      "murder 1 [2]\n",
      "much 3 [3, 8, 12]\n",
      "must 5 [3, 4, 5, 15]\n",
      "music 2 [9, 13]\n",
      "mr 1 [2]\n",
      "mrs 3 [2]\n",
      "messages 1 [6]\n",
      "mechanic 1 [9]\n",
      "medical 1 [11]\n",
      "meeting 1 [13]\n",
      "minor 1 [7]\n",
      "mind 1 [8]\n",
      "minutes 2 [8, 10]\n",
      "minimum 1 [15]\n",
      "mile 3 [10, 12, 14]\n",
      "miles 1 [8]\n",
      "million 2 [12, 15]\n",
      "millions 2 [9]\n",
      "mild 1 [11]\n",
      "milk 2 [14]\n",
      "milkplus 1 [14]\n",
      "middle 1 [9]\n",
      "might 4 [11, 12, 13]\n",
      "mcrap 1 [11]\n",
      "comic 1 [1]\n",
      "complications 1 [2]\n",
      "complied 1 [6]\n",
      "completely 1 [2]\n",
      "complex 1 [6]\n",
      "complain 1 [9]\n",
      "complaints 2 [13]\n",
      "company 4 [3, 4]\n",
      "come 5 [2, 5, 7, 9, 12]\n",
      "community 4 [12, 13]\n",
      "court 2 [1]\n",
      "course 2 [8, 12]\n",
      "couple 6 [1, 2, 10]\n",
      "could 2 [2, 10]\n",
      "county 1 [7]\n",
      "counts 1 [8]\n",
      "conversations 1 [2]\n",
      "convertible 1 [11]\n",
      "contestant 2 [3]\n",
      "continued 1 [13]\n",
      "condo 2 [5]\n",
      "condominium 1 [5]\n",
      "cons 1 [13]\n",
      "consisting 1 [12]\n",
      "considerate 1 [13]\n",
      "consideration 1 [13]\n",
      "concerned 1 [12]\n",
      "cone 1 [12]\n",
      "correct 1 [3]\n",
      "correctly 1 [3]\n",
      "correctional 1 [6]\n",
      "coin 1 [3]\n",
      "cost 2 [4, 15]\n",
      "collision 1 [7]\n",
      "collection 1 [10]\n",
      "color 2 [9, 10]\n",
      "colors 1 [12]\n",
      "coffee 3 [11]\n",
      "coffers 1 [13]\n",
      "city 11 [1, 13]\n",
      "cities 1 [1]\n",
      "cigarettes 1 [6]\n",
      "cigars 1 [6]\n",
      "car 6 [1, 3, 9, 12, 14]\n",
      "carpenter 1 [2]\n",
      "cars 7 [3, 7, 8, 13]\n",
      "carson 1 [12]\n",
      "carts 2 [7, 12]\n",
      "carriages 1 [12]\n",
      "careful 1 [13]\n",
      "came 7 [1, 2, 3, 9, 10, 12]\n",
      "california 3 [1, 5, 8]\n",
      "caliber 1 [10]\n",
      "called 3 [2, 7, 9]\n",
      "cat 5 [4]\n",
      "cataract 1 [2]\n",
      "candy 5 [2, 6]\n",
      "canine 1 [4]\n",
      "cane 1 [10]\n",
      "cans 1 [12]\n",
      "canton 2 [15]\n",
      "captured 1 [6]\n",
      "causing 1 [7]\n",
      "cause 2 [13]\n",
      "caused 1 [7]\n",
      "childless 1 [2]\n",
      "children 1 [14]\n",
      "changing 1 [2]\n",
      "change 2 [3, 14]\n",
      "changed 1 [9]\n",
      "channels 1 [3]\n",
      "chase 4 [7]\n",
      "charged 2 [7, 13]\n",
      "church 1 [3]\n",
      "choose 1 [3]\n",
      "cheap 1 [8]\n",
      "cheaper 1 [8]\n",
      "chef 1 [9]\n",
      "check 1 [14]\n",
      "close 1 [2]\n",
      "clone 4 [4]\n",
      "clones 1 [4]\n",
      "cloning 1 [4]\n",
      "clockwork 1 [9]\n",
      "clothing 1 [12]\n",
      "clicked 1 [3]\n",
      "clients 1 [4]\n",
      "club 2 [10]\n",
      "clubs 1 [12]\n",
      "clean 1 [12]\n",
      "cleared 3 [15]\n",
      "clearing 2 [15]\n",
      "cub 1 [12]\n",
      "cube 4 [3]\n",
      "cultured 2 [4]\n",
      "currently 1 [4]\n",
      "cut 3 [6, 15]\n",
      "cutting 1 [15]\n",
      "customers 2 [7, 8]\n",
      "cup 1 [11]\n",
      "cell 2 [3, 4]\n",
      "cells 2 [4]\n",
      "cents 2 [8, 14]\n",
      "certificates 1 [11]\n",
      "crossing 1 [3]\n",
      "crosswalk 1 [7]\n",
      "crazy 1 [5]\n",
      "crew 1 [12]\n",
      "crews 1 [13]\n",
      "creek 1 [12]\n",
      "cream 1 [12]\n",
      "created 1 [13]\n",
      "cycles 1 [5]\n",
      "book 2 [1]\n",
      "books 3 [1, 7]\n",
      "bookstore 1 [7]\n",
      "boots 1 [12]\n",
      "boon 1 [13]\n",
      "boy 3 [3, 12]\n",
      "born 1 [4]\n",
      "bought 6 [5, 8, 10, 14]\n",
      "bottle 1 [7]\n",
      "bottles 1 [12]\n",
      "boxes 1 [10]\n",
      "bowling 1 [12]\n",
      "bob 1 [14]\n",
      "become 1 [1]\n",
      "better 3 [3, 9]\n",
      "beloved 1 [4]\n",
      "believes 1 [4]\n",
      "beach 3 [5, 13]\n",
      "bedroom 3 [5, 13]\n",
      "berserk 1 [6]\n",
      "behind 1 [7]\n",
      "best 1 [9]\n",
      "blind 1 [2]\n",
      "blanket 1 [3]\n",
      "black 1 [10]\n",
      "block 3 [8]\n",
      "blocks 1 [7]\n",
      "blue 1 [14]\n",
      "bulbs 1 [2]\n",
      "bullet 1 [10]\n",
      "buy 7 [5, 10, 12, 14]\n",
      "bubble 1 [5]\n",
      "burst 2 [5]\n",
      "burger 2 [11]\n",
      "burn 1 [11]\n",
      "burned 1 [15]\n",
      "bus 5 [7, 10]\n",
      "bummer 1 [8]\n",
      "budget 1 [13]\n",
      "big 4 [3, 6, 12, 15]\n",
      "bigger 1 [3]\n",
      "bill 1 [3]\n",
      "biopsy 1 [4]\n",
      "births 1 [4]\n",
      "birder 1 [9]\n",
      "birding 1 [9]\n",
      "bizarre 1 [4]\n",
      "binoculars 1 [9]\n",
      "bicycles 1 [12]\n",
      "bikers 1 [15]\n",
      "bring 3 [5, 9, 13]\n",
      "brakes 1 [7]\n",
      "braked 1 [7]\n",
      "brand 1 [11]\n",
      "brown 1 [13]\n",
      "breakfast 1 [14]\n",
      "brush 1 [15]\n",
      "bartering 1 [6]\n",
      "barco 1 [8]\n",
      "barney 1 [10]\n",
      "barget 2 [14]\n",
      "back 5 [6, 7, 12, 14, 15]\n",
      "backbreaking 1 [12]\n",
      "basic 1 [6]\n",
      "basis 1 [13]\n",
      "bag 1 [12]\n",
      "bags 3 [6, 12]\n",
      "ban 2 [13]\n",
      "bang 1 [7]\n",
      "band 1 [9]\n",
      "baldwin 1 [10]\n",
      "balls 1 [12]\n",
      "batteries 1 [12]\n",
      "baby 1 [12]\n",
      "bay 1 [12]\n",
      "bad 1 [15]\n",
      "1 2 [3, 8]\n",
      "10 7 [1, 3, 5, 8, 10, 13, 14]\n",
      "100 1 [3]\n",
      "1000 3 [3]\n",
      "15 2 [13]\n",
      "150 2 [1, 12]\n",
      "12 1 [8]\n",
      "120 2 [3]\n",
      "120,000 2 [3]\n",
      "11 1 [15]\n",
      "110,000 1 [3]\n",
      "1993 1 [5]\n",
      "1992 1 [14]\n",
      "1995 1 [14]\n",
      "1,000 2 [10, 15]\n",
      "festival 6 [1]\n",
      "festivals 1 [1]\n",
      "featured 1 [1]\n",
      "fee 2 [1, 13]\n",
      "feel 3 [5]\n",
      "feet 1 [15]\n",
      "federal 3 [3, 15]\n",
      "felix 1 [4]\n",
      "female 2 [4, 11]\n",
      "following 2 [1, 10]\n",
      "food 6 [1, 2, 6]\n",
      "foods 1 [1]\n",
      "foot 1 [2]\n",
      "foothill 1 [14]\n",
      "found 2 [12]\n",
      "founders 1 [1]\n",
      "four 6 [9, 10, 13, 14, 15]\n",
      "forward 2 [15]\n",
      "free 2 [1, 11]\n",
      "freeway 1 [7]\n",
      "fresh 3 [2, 4, 11]\n",
      "francisco 1 [1]\n",
      "friends 4 [2, 5, 9]\n",
      "friendly 2 [2]\n",
      "friction 1 [13]\n",
      "fruit 1 [2]\n",
      "front 2 [7]\n",
      "fantastic 1 [1]\n",
      "fact 1 [2]\n",
      "facility 1 [4]\n",
      "failure 1 [7]\n",
      "fault 1 [11]\n",
      "families 1 [13]\n",
      "fixing 1 [2]\n",
      "first 6 [3, 4, 5, 9, 15]\n",
      "fire 5 [7, 12, 15]\n",
      "finally 2 [3, 10]\n",
      "find 1 [7]\n",
      "fine 2 [13]\n",
      "fines 1 [13]\n",
      "five 5 [5, 9, 12, 14, 15]\n",
      "fight 2 [6]\n",
      "fill 1 [8]\n",
      "filled 1 [12]\n",
      "fiction 1 [10]\n",
      "full 3 [2, 7, 12]\n",
      "future 2 [5, 15]\n",
      "furniture 1 [12]\n",
      "funds 1 [15]\n",
      "flipped 1 [3]\n",
      "sold 3 [1, 5]\n",
      "sought 1 [1]\n",
      "southern 1 [8]\n",
      "southland 1 [8]\n",
      "sometime 1 [5]\n",
      "sometimes 1 [13]\n",
      "something 1 [11]\n",
      "soap 2 [6]\n",
      "sofas 1 [12]\n",
      "social 1 [15]\n",
      "space 1 [1]\n",
      "sparing 1 [6]\n",
      "spanish 1 [10]\n",
      "sponsored 1 [1]\n",
      "sports 2 [3]\n",
      "spinning 1 [3]\n",
      "spill 1 [11]\n",
      "spilled 1 [11]\n",
      "spring 1 [7]\n",
      "spewed 1 [7]\n",
      "specter 1 [12]\n",
      "saturday 5 [1, 9, 10, 11, 12]\n",
      "san 1 [1]\n",
      "sandwich 1 [11]\n",
      "sandwiches 1 [1]\n",
      "santa 1 [5]\n",
      "said 27 [1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 15]\n",
      "say 4 [2, 5, 10, 15]\n",
      "says 4 [3, 4, 5]\n",
      "saying 1 [11]\n",
      "sad 1 [2]\n",
      "sam 10 [3]\n",
      "samantha 2 [9]\n",
      "saw 3 [5, 9]\n",
      "sale 2 [5, 14]\n",
      "savings 1 [8]\n",
      "save 2 [8, 12]\n",
      "saved 1 [10]\n",
      "saxophonist 2 [9]\n",
      "saxophone 3 [9]\n",
      "sara 8 [14]\n",
      "safely 1 [15]\n",
      "sunday 1 [1]\n",
      "succeed 1 [1]\n",
      "successfully 1 [4]\n",
      "suicide 1 [2]\n",
      "sue 1 [11]\n",
      "sued 1 [3]\n",
      "suv 3 [7]\n",
      "suffered 1 [11]\n",
      "summer 2 [13]\n",
      "surcharge 1 [13]\n",
      "sure 2 [13, 15]\n",
      "surround 1 [15]\n",
      "surrounded 1 [15]\n",
      "survive 1 [15]\n",
      "suggested 1 [13]\n",
      "sudden 1 [15]\n",
      "supposed 1 [15]\n",
      "seekers 1 [1]\n",
      "seem 2 [4, 9]\n",
      "seemed 2 [2]\n",
      "second 1 [3]\n",
      "secured 1 [6]\n",
      "security 1 [15]\n",
      "selected 1 [3]\n",
      "sell 2 [5, 12]\n",
      "seller 1 [5]\n",
      "sent 2 [4, 6]\n",
      "several 2 [7, 8]\n",
      "seven 1 [14]\n",
      "seatbelt 1 [7]\n",
      "seashell 2 [8]\n",
      "set 1 [12]\n",
      "shave 1 [1]\n",
      "shaking 1 [7]\n",
      "shapes 1 [12]\n",
      "shorter 1 [2]\n",
      "show 1 [8]\n",
      "shopping 2 [12, 14]\n",
      "sherman 3 [11]\n",
      "shiny 1 [12]\n",
      "sneaking 1 [1]\n",
      "six 3 [3, 15]\n",
      "sixth 1 [1]\n",
      "sixty 1 [3]\n",
      "sicker 1 [2]\n",
      "sickness 1 [2]\n",
      "sign 2 [3]\n",
      "silverware 1 [6]\n",
      "side 2 [7]\n",
      "sidewalks 1 [13]\n",
      "sizes 1 [12]\n",
      "since 1 [14]\n",
      "steadily 1 [2]\n",
      "steep 1 [4]\n",
      "steering 1 [7]\n",
      "state 5 [3, 5, 6, 15]\n",
      "station 4 [8]\n",
      "stationery 1 [6]\n",
      "stay 2 [5]\n",
      "started 1 [7]\n",
      "stars 1 [10]\n",
      "stand 1 [7]\n",
      "standing 1 [7]\n",
      "stain 1 [11]\n",
      "stop 4 [3, 7]\n",
      "stopped 2 [13, 14]\n",
      "store 4 [8, 10, 14]\n",
      "stored 1 [4]\n",
      "stolen 1 [7]\n",
      "still 5 [3, 5, 13, 14]\n",
      "street 4 [3, 7, 14]\n",
      "streets 1 [13]\n",
      "stretch 1 [12]\n",
      "streambed 1 [12]\n",
      "stray 1 [4]\n",
      "studio 1 [3]\n",
      "scared 1 [2]\n",
      "scalding 1 [11]\n",
      "scheduled 1 [12]\n",
      "scouts 2 [12]\n",
      "scoop 1 [12]\n",
      "slowly 1 [3]\n",
      "slam 1 [7]\n",
      "slammer 1 [9]\n",
      "slacks 1 [11]\n",
      "slumped 1 [7]\n",
      "slightly 1 [11]\n",
      "slice 1 [14]\n",
      "sleeping 1 [14]\n",
      "skyrocketing 1 [8]\n",
      "smiled 1 [12]\n",
      "smith 1 [14]\n",
      "year 17 [1, 2, 3, 4, 5, 7, 9, 11, 12, 15]\n",
      "years 16 [1, 2, 3, 5, 8, 10, 12, 13, 14, 15]\n",
      "yelling 1 [7]\n",
      "yellow 1 [12]\n",
      "yet 2 [9]\n",
      "yield 1 [7]\n",
      "young 2 [8, 11]\n",
      "half 6 [1, 2, 4, 6, 12]\n",
      "halloween 1 [2]\n",
      "halt 1 [7]\n",
      "hamburgers 1 [1]\n",
      "hamilton 1 [15]\n",
      "hawaiian 1 [1]\n",
      "handed 1 [2]\n",
      "handle 1 [4]\n",
      "happen 1 [7]\n",
      "happened 1 [2]\n",
      "happy 2 [9]\n",
      "hauled 2 [6, 12]\n",
      "hazard 1 [15]\n",
      "hour 3 [1, 3, 7]\n",
      "hours 5 [3, 9, 12]\n",
      "hourly 1 [13]\n",
      "house 5 [2, 5, 13]\n",
      "housing 3 [5]\n",
      "hospital 3 [3, 7]\n",
      "home 5 [9, 10, 14]\n",
      "homebuyers 2 [5]\n",
      "homeowner 3 [13]\n",
      "homeowners 3 [13]\n",
      "homes 2 [13, 15]\n",
      "hope 1 [12]\n",
      "hopes 2 [5]\n",
      "hot 2 [11, 14]\n",
      "hotel 1 [7]\n",
      "however 2 [7, 13]\n",
      "howard 2 [9]\n",
      "horsetrail 1 [8]\n",
      "hold 1 [13]\n",
      "holiday 1 [14]\n",
      "husband 1 [1]\n",
      "hurriedly 1 [7]\n",
      "huge 1 [13]\n",
      "help 2 [2, 15]\n",
      "held 1 [6]\n",
      "heard 1 [2]\n",
      "head 1 [9]\n",
      "heads 1 [3]\n",
      "headaches 1 [13]\n",
      "herman 1 [11]\n",
      "hill 1 [3]\n",
      "hit 1 [3]\n",
      "high 3 [5, 13, 15]\n",
      "higher 2 [5, 8]\n",
      "hire 1 [11]\n",
      "hydrant 2 [7]\n",
      "new 11 [4, 5, 9, 10, 11]\n",
      "newspaper 2 [1, 7]\n",
      "newsstand 1 [7]\n",
      "newest 1 [11]\n",
      "next 5 [2, 12, 13, 15]\n",
      "neighbor 6 [2, 9]\n",
      "neighborhood 2 [2, 8]\n",
      "neighbors 5 [9, 13]\n",
      "never 1 [2]\n",
      "nevertheless 1 [4]\n",
      "near 1 [5]\n",
      "nearby 1 [15]\n",
      "necessary 2 [6, 15]\n",
      "negotiations 1 [6]\n",
      "nobody 1 [1]\n",
      "notify 1 [4]\n",
      "north 3 [7, 10]\n",
      "northville 1 [11]\n",
      "noise 2 [13]\n",
      "nonfat 1 [14]\n",
      "nonflammable 1 [15]\n",
      "night 3 [2, 13]\n",
      "nice 5 [2, 4, 5, 10]\n",
      "nicest 1 [2]\n",
      "nitrogen 1 [4]\n",
      "nine 2 [12]\n",
      "ninety 1 [15]\n",
      "number 4 [3]\n",
      "numbers 1 [3]\n",
      "nutritious 1 [14]\n",
      "nationwide 1 [5]\n",
      "name 1 [10]\n",
      "names 1 [13]\n",
      "nancy 1 [14]\n",
      "7 1 [1]\n",
      "70 1 [2]\n",
      "70,000 2 [1, 5]\n",
      "75 1 [6]\n",
      "75,000 1 [1]\n",
      "74 1 [2]\n",
      "76 1 [3]\n",
      "79 1 [11]\n",
      "280 1 [1]\n",
      "20 3 [2, 8]\n",
      "20,000 1 [5]\n",
      "200 1 [13]\n",
      "25 1 [3]\n",
      "25,000 2 [4]\n",
      "230,000 1 [5]\n",
      "24 1 [7]\n",
      "2.22 1 [8]\n",
      "2.09 1 [8]\n",
      "2.14 1 [8]\n",
      "2,000 1 [13]\n",
      "29.95 1 [14]\n",
      "9 2 [11, 14]\n",
      "90 1 [1]\n",
      "9,000 1 [3]\n",
      "99 1 [8]\n",
      "question 2 [1, 5]\n",
      "quell 1 [6]\n",
      "quieter 2 [2]\n",
      "quite 1 [11]\n",
      "kinds 2 [1, 2]\n",
      "kids 3 [2, 14]\n",
      "kitten 3 [4]\n",
      "kittens 1 [4]\n",
      "kick 1 [5]\n",
      "killing 1 [8]\n",
      "knew 2 [1, 9]\n",
      "knee 1 [10]\n",
      "know 4 [5, 13, 15]\n",
      "ice 2 [1, 12]\n",
      "idea 2 [1, 5]\n",
      "ideas 1 [15]\n",
      "interest 2 [2, 5]\n",
      "insurance 1 [3]\n",
      "inc 4 [4]\n",
      "inconsiderate 2 [9, 13]\n",
      "income 1 [13]\n",
      "incomes 1 [13]\n",
      "inevitably 1 [5]\n",
      "inmates 3 [6]\n",
      "influence 1 [7]\n",
      "inferno 1 [15]\n",
      "invite 1 [10]\n",
      "injured 1 [11]\n",
      "immediately 1 [2]\n",
      "implanted 1 [4]\n",
      "item 2 [6]\n",
      "items 2 [6]\n",
      "irritating 1 [9]\n",
      "issued 1 [12]\n",
      "drinks 2 [1]\n",
      "drive 5 [8, 11]\n",
      "driver 9 [7]\n",
      "drivers 1 [7]\n",
      "driving 1 [7]\n",
      "drizzle 1 [12]\n",
      "drove 3 [1, 11, 14]\n",
      "dropped 1 [10]\n",
      "drought 1 [15]\n",
      "dreamily 1 [3]\n",
      "deal 1 [6]\n",
      "deals 1 [1]\n",
      "death 1 [2]\n",
      "delivered 1 [2]\n",
      "delivers 1 [4]\n",
      "delightful 2 [9]\n",
      "decision 2 [3, 5]\n",
      "decide 2 [5]\n",
      "decided 1 [10]\n",
      "desperate 1 [5]\n",
      "destroyed 1 [15]\n",
      "demanding 1 [6]\n",
      "demands 1 [6]\n",
      "details 1 [10]\n",
      "debris 2 [12]\n",
      "debating 1 [13]\n",
      "department 1 [14]\n",
      "departments 1 [12]\n",
      "died 1 [2]\n",
      "dies 1 [4]\n",
      "diabetic 1 [2]\n",
      "diabetes 1 [2]\n",
      "disease 1 [2]\n",
      "discovered 1 [6]\n",
      "director 3 [5, 10, 12]\n",
      "directions 1 [7]\n",
      "dirt 1 [15]\n",
      "dining 1 [6]\n",
      "dictionaries 1 [6]\n",
      "different 3 [7, 9]\n",
      "dom 2 [2]\n",
      "dominic 1 [2]\n",
      "door 4 [2, 6, 9, 14]\n",
      "doors 1 [6]\n",
      "donor 1 [4]\n",
      "donna 1 [5]\n",
      "done 2 [12]\n",
      "donate 1 [12]\n",
      "dozen 1 [4]\n",
      "dogs 2 [4]\n",
      "downtown 2 [7]\n",
      "dolls 1 [12]\n",
      "dollars 1 [12]\n",
      "day 5 [5, 9, 12]\n",
      "days 1 [4]\n",
      "daily 1 [6]\n",
      "damage 1 [7]\n",
      "us 1 [1]\n",
      "use 1 [12]\n",
      "used 6 [2, 8, 10]\n",
      "unpredictable 1 [1]\n",
      "unemployed 1 [3]\n",
      "uneventful 1 [7]\n",
      "unlucky 1 [3]\n",
      "uninjured 1 [7]\n",
      "unhappy 1 [9]\n",
      "underbrush 2 [15]\n",
      "unfortunately 1 [15]\n",
      "ups 1 [9]\n",
      "upset 1 [11]\n",
      "5 2 [5, 8]\n",
      "50 6 [2, 3, 4, 14]\n",
      "500 2 [3, 12]\n",
      "50,000 1 [4]\n",
      "510,000 1 [5]\n",
      "60 1 [4]\n",
      "6,000 1 [15]\n",
      "65 1 [15]\n",
      "87 1 [8]\n",
      "3 1 [15]\n",
      "30 3 [8, 10, 14]\n",
      "300 1 [10]\n",
      "3037 1 [14]\n",
      "38 1 [10]\n",
      "39.95 1 [14]\n",
      "3.50 1 [14]\n",
      "00 1 [11]\n",
      "4,000 2 [15]\n"
     ]
    }
   ],
   "source": [
    "inverted_index.print_words()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e26ec1bab8a1abd2e5a712081492a9566af19802245f28e584dda2f9826c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
