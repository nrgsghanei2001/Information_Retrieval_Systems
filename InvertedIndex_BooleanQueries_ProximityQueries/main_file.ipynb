{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Files\n",
    "\n",
    "This part opens a directory and reads all of files in it and saves their context in a list as string.\n",
    "It also saves the names of files to use later for result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the documents are: \n",
      "A Festival of Books - A Murder-Suicide - Better To Be Unlucky - Cloning Pets - Crazy Housing Prices - Food Fight Erupted in Prison - Freeway Chase Ends at Newsstand - Gasoline Prices Hit Record High - Happy and Unhappy Renters - Jerry Decided To Buy a Gun - Man Injured at Fast Food Place - Pulling Out Nine Tons of Trash - Rentals at the Oceanside Community - Sara Went Shopping - Trees Are a Threat - "
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# where the folder of all docs is\n",
    "path = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\documents\" \n",
    "os.chdir(path) \n",
    "name_of_docs = []  # name of documents\n",
    "list_of_docs = []  # list of context of documents\n",
    "\n",
    "for file in os.listdir(): \n",
    "    file_path = f\"{path}\\{file}\"\n",
    "    name_of_docs.append(file[:-4])\n",
    "    doc = \"\" \n",
    "    # read the file context and put it in list\n",
    "    with open(file_path, 'r') as f: \n",
    "        doc = f.read()\n",
    "        list_of_docs.append(doc)\n",
    "\n",
    "print(\"All of the documents are: \")\n",
    "for name in name_of_docs:\n",
    "    print(name, end=\" - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing\n",
    "\n",
    "In this part we go through all documents and tokenize their terms\n",
    "and save the tokens of each document in a 2D array.\n",
    "we can also stem them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  4114\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "all_tokens = []         # a list to save all of tokens of all documents\n",
    "for i, doc in enumerate(list_of_docs):\n",
    "    lower_doc = doc.lower()              # make all of contexts lower case\n",
    "    list_of_docs[i] = lower_doc          \n",
    "    tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_docs[i])   # tokenize the text with regex\n",
    "    ## Stem the tokens(This part is ignored for this project)\n",
    "    # stemmer = nltk.stem.PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    all_tokens.append(tokens)\n",
    "\n",
    "sum_of_tokens = 0\n",
    "for doc in all_tokens:\n",
    "    sum_of_tokens += len(doc)\n",
    "print(\"Total number of tokens: \", sum_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Unneccessary Tokens\n",
    "\n",
    "In this part we try to delete useless tokens like single characters and stopwords from tokens.\n",
    "skip the stop words for now and delete them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens after removing single chars:  3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(all_tokens):\n",
    "    new_tokens = []\n",
    "    for token in doc:                           # delete all of single character tokens except numbers from token list\n",
    "        if len(token) < 2 and token.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    all_tokens[i] = new_tokens\n",
    "\n",
    "sum_of_tokens1 = 0\n",
    "for doc in all_tokens:\n",
    "    sum_of_tokens1 += len(doc)\n",
    "print(\"Total number of tokens after removing single chars: \", sum_of_tokens1)\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure for Inverted Index\n",
    "\n",
    "Build a trie tree to save all the terms and for each term save its posting list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.posting_list = []\n",
    "        self.is_word = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    # function to insert new terms to the trie tree\n",
    "    def insert(self, word, docID):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, add it as a new child node\n",
    "            if char not in current_node.children:\n",
    "                current_node.children[char] = TrieNode()\n",
    "            current_node = current_node.children[char]\n",
    "        # Mark the end of the word\n",
    "        current_node.is_word = True\n",
    "        # add document ID to posting list of the added term\n",
    "        current_node.posting_list.append(docID)\n",
    "\n",
    "    # function to check if a term is present in the tree or not\n",
    "    def search(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, the word is not in the trie\n",
    "            if char not in current_node.children:\n",
    "                return False\n",
    "            current_node = current_node.children[char]\n",
    "        return current_node.is_word\n",
    "\n",
    "    # function to return the term's node when it is present in the tree\n",
    "    def get(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            current_node = current_node.children[char]\n",
    "        # Return the last node itself\n",
    "        return current_node\n",
    "\n",
    "    # A function to print all the terms in the trie\n",
    "    def print_words(self, node=None, prefix=\"\"):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        # If the node is a word, print the prefix\n",
    "        if node.is_word:\n",
    "            print(prefix, node.posting_list)\n",
    "        # Loop through each child node\n",
    "        for char, child in node.children.items():\n",
    "            # Recursively call the function with the child node and the updated prefix\n",
    "            self.print_words(child, prefix + char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure for Positional Index and Permuterm Index\n",
    "\n",
    "build a trie tree to save all terms(and their permuterms) and their posting list with position of term in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode2:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.posting_list = {}\n",
    "        self.is_word = False\n",
    "\n",
    "class Trie2:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode2()\n",
    "\n",
    "    # function to insert new terms to the trie tree\n",
    "    def insert2(self, word, docID, position):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, add it as a new child node\n",
    "            if char not in current_node.children:\n",
    "                current_node.children[char] = TrieNode2()\n",
    "            current_node = current_node.children[char]\n",
    "        # Mark the end of the word\n",
    "        current_node.is_word = True\n",
    "        # add docId and position of term in documnet to the posting list\n",
    "        keysList = list(current_node.posting_list.keys())\n",
    "        if docID not in keysList:  # doc ID is not in the term's posting list, add docId and postion of term to posting list\n",
    "            current_node.posting_list.update({docID:[position]})\n",
    "        else:    # doc ID is currently in the term's posting list, just add a new position\n",
    "            current_node.posting_list.get(docID).append(position)\n",
    "\n",
    "    # function to check if a term is present in the tree or not\n",
    "    def search2(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, the word is not in the trie\n",
    "            if char not in current_node.children:\n",
    "                return False\n",
    "            current_node = current_node.children[char]\n",
    "        return current_node.is_word\n",
    "\n",
    "    # function to return the term's node when it is present in the tree\n",
    "    def get2(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            if char not in current_node.children:\n",
    "                return None\n",
    "            current_node = current_node.children[char]\n",
    "        # Return the last node\n",
    "        return current_node\n",
    "\n",
    "    # A function to print all the terms in the trie\n",
    "    def print_words2(self, node=None, prefix=\"\"):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        # If the node is a word, print the prefix\n",
    "        if node.is_word:\n",
    "            print(prefix, node.posting_list)\n",
    "\n",
    "        for char, child in node.children.items():\n",
    "            # Recursively call the function with the child node and the updated prefix\n",
    "            self.print_words2(child, prefix + char)\n",
    "\n",
    "    # A function to find all the words that match a wildcard query in a trie\n",
    "    def find_words(self, prefix):\n",
    "        # list to store the matching words\n",
    "        words = []\n",
    "        # Find the node that corresponds to the prefix in the trie\n",
    "        node = self.get2(prefix)\n",
    "        # If the node exists, recursively collect the words that start with the prefix\n",
    "        if node:\n",
    "            collect_words(node, prefix, words)\n",
    "        return words\n",
    "\n",
    "# a function to collect all terms with some given prefix in the trie tree\n",
    "def collect_words(node, prefix, words):\n",
    "    # If the node is a word, append the prefix to the list of words\n",
    "    if node.is_word:\n",
    "        words.append(prefix)\n",
    "    # Loop through each child node\n",
    "    for char, child in node.children.items():\n",
    "        # Recursively call the function with the child node and the updated prefix\n",
    "        collect_words(child, prefix + char, words)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Inverted Index and Positional Index\n",
    "\n",
    "for all tokens fill trie tree of inverted index with terms and doc ids and trie tree of positional index with terms, doc ids and positions.\n",
    "remove stop words in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens after removing stop words:  2226\n"
     ]
    }
   ],
   "source": [
    "inverted_index = Trie()       # trie tree for saving inverted index\n",
    "positional_index = Trie2()    # trie tree for saving positional index and permuterm index\n",
    "\n",
    "sum_of_tokens2 = 0\n",
    "for i, doc in enumerate(all_tokens):\n",
    "    count = 0\n",
    "    for term in doc:\n",
    "        count += 1\n",
    "        if term not in stop_words:                     # skip the stop words and don't save them in tries\n",
    "            sum_of_tokens2 += 1\n",
    "            if inverted_index.search(term) == False:   # if term is not currently in trie, insert it\n",
    "                inverted_index.insert(term, i)\n",
    "            else:                                      # if term is currently in trie, \n",
    "                node = inverted_index.get(term)\n",
    "                if i not in node.posting_list:         # just if it is in a new doc, add doc id to posting list\n",
    "                    node.posting_list.append(i)\n",
    "\n",
    "            positional_index.insert2(term, i, count)   # insert term and its doc id and position to positional index trie\n",
    "\n",
    "\n",
    "print(\"Total number of tokens after removing stop words: \", sum_of_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Query\n",
    "\n",
    "getting the query and tokenize and lower it the same as the documents.\n",
    "check the boolean query, if it is NOT, find the documents that the mentioned term is not appeared in them,\n",
    "for AND, intersect the incidence lists of first and second term and for OR union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is: \n",
      "2 Better To Be Unlucky\n",
      "4 Crazy Housing Prices\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter a boolean query: \")\n",
    "query = query.lower()                    # make the query lower case\n",
    "query_list = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', query)   # tokenize the query like documents texts\n",
    "\n",
    "if len(query_list) == 2:  # not\n",
    "    if inverted_index.search(query_list[1]):      # if given term is in inverted index\n",
    "        list1 = inverted_index.get(query_list[1]).posting_list  # get the posting list of given term\n",
    "        result = []\n",
    "        n = len(name_of_docs)\n",
    "        for i in range(n):              # all of the docs that the term is not in them is the result\n",
    "            if i not in list1:\n",
    "                result.append(i)\n",
    "    else:\n",
    "        print(\"Term not found.\")\n",
    "else:      # or / and     \n",
    "    list1, list2 = [], [] \n",
    "    if inverted_index.search(query_list[0]):    # get the posting lists of the given terms\n",
    "        list1 = inverted_index.get(query_list[0]).posting_list\n",
    "    if inverted_index.search(query_list[2]):\n",
    "        list2 = inverted_index.get(query_list[2]).posting_list     \n",
    "\n",
    "    if \"and\" in query_list:\n",
    "        result = list(set(list1) & set(list2))\n",
    "    elif \"or\" in query_list:\n",
    "        result = list(set(list1) | set(list2))\n",
    "\n",
    "print(\"The result is: \")\n",
    "for r in result:\n",
    "    print(r, name_of_docs[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get proximity query\n",
    "\n",
    "get the query and tokenize it and separate the number in the middle of it.\n",
    "search the documents that have both terms and find out if the distance of position of the 2 terms is less than given number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is: \n",
      "8 Happy and Unhappy Renters\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter a proximity query: \")\n",
    "query = query.lower()          # make the query lower case\n",
    "query_list = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', query)   # tokenize the query like documents texts\n",
    "query_list[2] = int(query_list[2])   # find the proxomity number\n",
    "query_list = query_list[:1]+query_list[2:]   # delete the word \"near\" from the query tokens(useless)\n",
    "\n",
    "list1, list2 = [], []\n",
    "if positional_index.search2(query_list[0]):   # find the posting lists of each term\n",
    "    list1 = list(positional_index.get2(query_list[0]).posting_list.keys())\n",
    "if positional_index.search2(query_list[2]):\n",
    "    list2 = list(positional_index.get2(query_list[2]).posting_list.keys())\n",
    "\n",
    "intersect_docs = list(set(list1) & set(list2))   # find the documents that both of terms are in\n",
    "result = []\n",
    "for i in intersect_docs:\n",
    "    a = positional_index.get2(query_list[0]).posting_list.get(i)   # find the position of occuring terms in the common documents\n",
    "    b = positional_index.get2(query_list[2]).posting_list.get(i)\n",
    "    for j in a:\n",
    "        for k in b:\n",
    "            if abs(j-k)-1 <= query_list[1]:   # find the distance of terms in each common document and check it with limit\n",
    "                if i not in result:\n",
    "                    result.append(i)\n",
    "print(\"the result is: \")\n",
    "for r in result:\n",
    "    print(r, name_of_docs[r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e26ec1bab8a1abd2e5a712081492a9566af19802245f28e584dda2f9826c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
