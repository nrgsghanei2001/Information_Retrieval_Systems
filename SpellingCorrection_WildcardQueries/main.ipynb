{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Files\n",
    "\n",
    "This part opens a directory and reads all of files in it and saves their context in a list as string.\n",
    "It also saves the names of files to use later for result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the documents are: \n",
      "A Festival of Books - A Murder-Suicide - Better To Be Unlucky - Cloning Pets - Crazy Housing Prices - Food Fight Erupted in Prison - Freeway Chase Ends at Newsstand - Gasoline Prices Hit Record High - Happy and Unhappy Renters - Jerry Decided To Buy a Gun - Man Injured at Fast Food Place - Pulling Out Nine Tons of Trash - Rentals at the Oceanside Community - Sara Went Shopping - Trees Are a Threat - "
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# where the folder of all docs is\n",
    "path = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\documents\" \n",
    "os.chdir(path) \n",
    "name_of_docs = []  # name of documents\n",
    "list_of_docs = []  # list of context of documents\n",
    "\n",
    "for file in os.listdir(): \n",
    "    file_path = f\"{path}\\{file}\"\n",
    "    name_of_docs.append(file[:-4])\n",
    "    doc = \"\" \n",
    "    # read the file context and put it in list\n",
    "    with open(file_path, 'r') as f: \n",
    "        doc = f.read()\n",
    "        list_of_docs.append(doc)\n",
    "\n",
    "print(\"All of the documents are: \")\n",
    "for name in name_of_docs:\n",
    "    print(name, end=\" - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing\n",
    "\n",
    "In this part we go through all documents and tokenize their terms\n",
    "and save the tokens of each document in a 2D array.\n",
    "we can also stem them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  4114\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "all_tokens = []         # a list to save all of tokens of all documents\n",
    "for i, doc in enumerate(list_of_docs):\n",
    "    lower_doc = doc.lower()              # make all of contexts lower case\n",
    "    list_of_docs[i] = lower_doc          \n",
    "    tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_docs[i])   # tokenize the text with regex\n",
    "    ## Stem the tokens(This part is ignored for this project)\n",
    "    # stemmer = nltk.stem.PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    all_tokens.append(tokens)\n",
    "\n",
    "sum_of_tokens = 0\n",
    "for doc in all_tokens:\n",
    "    sum_of_tokens += len(doc)\n",
    "print(\"Total number of tokens: \", sum_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Unneccessary Tokens\n",
    "\n",
    "In this part we try to delete useless tokens like single characters and stopwords from tokens.\n",
    "skip the stop words for now and delete them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens after removing single chars:  3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(all_tokens):\n",
    "    new_tokens = []\n",
    "    for token in doc:                           # delete all of single character tokens except numbers from token list\n",
    "        if len(token) < 2 and token.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    all_tokens[i] = new_tokens\n",
    "\n",
    "sum_of_tokens1 = 0\n",
    "for doc in all_tokens:\n",
    "    sum_of_tokens1 += len(doc)\n",
    "print(\"Total number of tokens after removing single chars: \", sum_of_tokens1)\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure for Inverted Index\n",
    "\n",
    "Build a trie tree to save all the terms and for each term save its posting list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.posting_list = []\n",
    "        self.is_word = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    # function to insert new terms to the trie tree\n",
    "    def insert(self, word, docID):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, add it as a new child node\n",
    "            if char not in current_node.children:\n",
    "                current_node.children[char] = TrieNode()\n",
    "            current_node = current_node.children[char]\n",
    "        # Mark the end of the word\n",
    "        current_node.is_word = True\n",
    "        # add document ID to posting list of the added term\n",
    "        current_node.posting_list.append(docID)\n",
    "\n",
    "    # function to check if a term is present in the tree or not\n",
    "    def search(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, the word is not in the trie\n",
    "            if char not in current_node.children:\n",
    "                return False\n",
    "            current_node = current_node.children[char]\n",
    "        return current_node.is_word\n",
    "\n",
    "    # function to return the term's node when it is present in the tree\n",
    "    def get(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            current_node = current_node.children[char]\n",
    "        # Return the last node itself\n",
    "        return current_node\n",
    "\n",
    "    # A function to print all the terms in the trie\n",
    "    def print_words(self, node=None, prefix=\"\"):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        # If the node is a word, print the prefix\n",
    "        if node.is_word:\n",
    "            print(prefix, node.posting_list)\n",
    "        # Loop through each child node\n",
    "        for char, child in node.children.items():\n",
    "            # Recursively call the function with the child node and the updated prefix\n",
    "            self.print_words(child, prefix + char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure for Positional Index and Permuterm Index\n",
    "\n",
    "build a trie tree to save all terms(and their permuterms) and their posting list with position of term in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode2:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.posting_list = {}\n",
    "        self.is_word = False\n",
    "\n",
    "class Trie2:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode2()\n",
    "\n",
    "    # function to insert new terms to the trie tree\n",
    "    def insert2(self, word, docID, position):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, add it as a new child node\n",
    "            if char not in current_node.children:\n",
    "                current_node.children[char] = TrieNode2()\n",
    "            current_node = current_node.children[char]\n",
    "        # Mark the end of the word\n",
    "        current_node.is_word = True\n",
    "        # add docId and position of term in documnet to the posting list\n",
    "        keysList = list(current_node.posting_list.keys())\n",
    "        if docID not in keysList:  # doc ID is not in the term's posting list, add docId and postion of term to posting list\n",
    "            current_node.posting_list.update({docID:[position]})\n",
    "        else:    # doc ID is currently in the term's posting list, just add a new position\n",
    "            current_node.posting_list.get(docID).append(position)\n",
    "\n",
    "    # function to check if a term is present in the tree or not\n",
    "    def search2(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            # If the character is not in the current node's children, the word is not in the trie\n",
    "            if char not in current_node.children:\n",
    "                return False\n",
    "            current_node = current_node.children[char]\n",
    "        return current_node.is_word\n",
    "\n",
    "    # function to return the term's node when it is present in the tree\n",
    "    def get2(self, word):\n",
    "        current_node = self.root\n",
    "        for char in word:\n",
    "            if char not in current_node.children:\n",
    "                return None\n",
    "            current_node = current_node.children[char]\n",
    "        # Return the last node\n",
    "        return current_node\n",
    "\n",
    "    # A function to print all the terms in the trie\n",
    "    def print_words2(self, node=None, prefix=\"\"):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        # If the node is a word, print the prefix\n",
    "        if node.is_word:\n",
    "            print(prefix, node.posting_list)\n",
    "\n",
    "        for char, child in node.children.items():\n",
    "            # Recursively call the function with the child node and the updated prefix\n",
    "            self.print_words2(child, prefix + char)\n",
    "\n",
    "    # A function to find all the words that match a wildcard query in a trie\n",
    "    def find_words(self, prefix):\n",
    "        # list to store the matching words\n",
    "        words = []\n",
    "        # Find the node that corresponds to the prefix in the trie\n",
    "        node = self.get2(prefix)\n",
    "        # If the node exists, recursively collect the words that start with the prefix\n",
    "        if node:\n",
    "            collect_words(node, prefix, words)\n",
    "        return words\n",
    "\n",
    "# a function to collect all terms with some given prefix in the trie tree\n",
    "def collect_words(node, prefix, words):\n",
    "    # If the node is a word, append the prefix to the list of words\n",
    "    if node.is_word:\n",
    "        words.append(prefix)\n",
    "    # Loop through each child node\n",
    "    for char, child in node.children.items():\n",
    "        # Recursively call the function with the child node and the updated prefix\n",
    "        collect_words(child, prefix + char, words)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Inverted Index and Positional Index\n",
    "\n",
    "for all tokens fill trie tree of inverted index with terms and doc ids and trie tree of positional index with terms, doc ids and positions.\n",
    "remove stop words in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens after removing stop words:  2226\n"
     ]
    }
   ],
   "source": [
    "inverted_index = Trie()       # trie tree for saving inverted index\n",
    "positional_index = Trie2()    # trie tree for saving positional index and permuterm index\n",
    "\n",
    "sum_of_tokens2 = 0\n",
    "for i, doc in enumerate(all_tokens):\n",
    "    count = 0\n",
    "    for term in doc:\n",
    "        count += 1\n",
    "        if term not in stop_words:                     # skip the stop words and don't save them in tries\n",
    "            sum_of_tokens2 += 1\n",
    "            if inverted_index.search(term) == False:   # if term is not currently in trie, insert it\n",
    "                inverted_index.insert(term, i)\n",
    "            else:                                      # if term is currently in trie, \n",
    "                node = inverted_index.get(term)\n",
    "                if i not in node.posting_list:         # just if it is in a new doc, add doc id to posting list\n",
    "                    node.posting_list.append(i)\n",
    "\n",
    "            positional_index.insert2(term, i, count)   # insert term and its doc id and position to positional index trie\n",
    "\n",
    "\n",
    "print(\"Total number of tokens after removing stop words: \", sum_of_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Query\n",
    "\n",
    "getting the query and tokenize and lower it the same as the documents.\n",
    "check the boolean query, if it is NOT, find the documents that the mentioned term is not appeared in them,\n",
    "for AND, intersect the incidence lists of first and second term and for OR union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is: \n",
      "2 Better To Be Unlucky\n",
      "4 Crazy Housing Prices\n"
     ]
    }
   ],
   "source": [
    "def boolean_query(query):\n",
    "    query = query.lower()                    # make the query lower case\n",
    "    query_list = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', query)   # tokenize the query like documents texts\n",
    "    if len(query_list) == 2:  # not\n",
    "        if inverted_index.search(query_list[1]):      # if given term is in inverted index\n",
    "            list1 = inverted_index.get(query_list[1]).posting_list  # get the posting list of given term\n",
    "            result = []\n",
    "            n = len(name_of_docs)\n",
    "            for i in range(n):              # all of the docs that the term is not in them is the result\n",
    "                if i not in list1:\n",
    "                    result.append(i)\n",
    "        else:\n",
    "            print(\"Term not found.\")\n",
    "    else:      # or / and     \n",
    "        list1, list2 = [], [] \n",
    "        if inverted_index.search(query_list[0]):    # get the posting lists of the given terms\n",
    "            list1 = inverted_index.get(query_list[0]).posting_list\n",
    "        if inverted_index.search(query_list[2]):\n",
    "            list2 = inverted_index.get(query_list[2]).posting_list     \n",
    "\n",
    "        if \"and\" in query_list:\n",
    "            result = list(set(list1) & set(list2))\n",
    "        elif \"or\" in query_list:\n",
    "            result = list(set(list1) | set(list2))\n",
    "\n",
    "    print(\"The result is: \")\n",
    "    for r in result:\n",
    "        print(r, name_of_docs[r])\n",
    "\n",
    "query = input(\"Enter a boolean query: \")\n",
    "boolean_query(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get proximity query\n",
    "\n",
    "get the query and tokenize it and separate the number in the middle of it.\n",
    "search the documents that have both terms and find out if the distance of position of the 2 terms is less than given number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is: \n",
      "8 Happy and Unhappy Renters\n"
     ]
    }
   ],
   "source": [
    "def proximity_query(query):\n",
    "    query = query.lower()          # make the query lower case\n",
    "    query_list = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', query)   # tokenize the query like documents texts\n",
    "    query_list[2] = int(query_list[2])   # find the proxomity number\n",
    "    query_list = query_list[:1]+query_list[2:]   # delete the word \"near\" from the query tokens(useless)\n",
    "\n",
    "    list1, list2 = [], []\n",
    "    if positional_index.search2(query_list[0]):   # find the posting lists of each term\n",
    "        list1 = list(positional_index.get2(query_list[0]).posting_list.keys())\n",
    "    if positional_index.search2(query_list[2]):\n",
    "        list2 = list(positional_index.get2(query_list[2]).posting_list.keys())\n",
    "\n",
    "    intersect_docs = list(set(list1) & set(list2))   # find the documents that both of terms are in\n",
    "    result = []\n",
    "    for i in intersect_docs:\n",
    "        a = positional_index.get2(query_list[0]).posting_list.get(i)   # find the position of occuring terms in the common documents\n",
    "        b = positional_index.get2(query_list[2]).posting_list.get(i)\n",
    "        for j in a:\n",
    "            for k in b:\n",
    "                if abs(j-k)-1 <= query_list[1]:   # find the distance of terms in each common document and check it with limit\n",
    "                    if i not in result:\n",
    "                        result.append(i)\n",
    "    print(\"the result is: \")\n",
    "    for r in result:\n",
    "        print(r, name_of_docs[r])\n",
    "\n",
    "query = input(\"Enter a proximity query: \")\n",
    "proximity_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find permuterms of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(word):\n",
    "    return word + \"$\"\n",
    "\n",
    "# A function to generate a permuterm index for a word\n",
    "def permuterm(word):\n",
    "    # Rotate the word and append $\n",
    "    word = rotate(word)\n",
    "    # store the permutations\n",
    "    permutations = []\n",
    "    for i in range(len(word)):\n",
    "        permutation = word[i:] + word[:i]\n",
    "        permutations.append(permutation)\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute soundex to restrict domain of calculating distance for spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_soundex(word):\n",
    "    # Retain the first letter and convert it to uppercase\n",
    "    code = word[0].upper()\n",
    "    # Remove any vowels, H, W, or Y, unless they are the first letter\n",
    "    word = word[0] + \"\".join(char for char in word[1:] if char not in \"aehiouwy\")\n",
    "    # Replace the remaining consonants with their corresponding digits\n",
    "    for char, digit in ((\"b\", \"1\"),(\"f\", \"1\"),(\"p\", \"1\"),(\"v\", \"1\"),\n",
    "    (\"c\", \"2\"), (\"g\", \"2\"),(\"j\", \"2\"),(\"k\", \"2\"),(\"q\", \"2\"),(\"s\", \"2\"),(\"x\", \"2\"),(\"z\", \"2\"),\n",
    "    (\"d\", \"3\"), (\"t\", \"3\"),(\"l\", \"4\"), (\"m\", \"5\"), (\"n\", \"5\"),(\"r\", \"6\")):\n",
    "        word = word.replace(char, digit)\n",
    "    # Remove any adjacent digits that are the same\n",
    "    for digit in \"123456\":\n",
    "        word = word.replace(digit + digit, digit)\n",
    "    # Pad the code with zeros if it has less than four characters\n",
    "    code += word[1:]\n",
    "    code = code.ljust(4, \"0\")\n",
    "    # Truncate the code if it has more than four characters\n",
    "    code = code[:4]\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build permuterm index \n",
    "\n",
    "insert each term and all of its permutations to trie and create a 2D array to save the soundex of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuterm_index = Trie2()\n",
    "soundex_map = [[],[]]\n",
    "for i, doc in enumerate(all_tokens):\n",
    "    count = 0\n",
    "    for term in doc:\n",
    "        count += 1\n",
    "        if term not in stop_words:\n",
    "            code = compute_soundex(term)         # compute the soundex of term\n",
    "            if term not in soundex_map[0]:\n",
    "                soundex_map[0].append(term)      # insert each term with its soundex to the array\n",
    "                soundex_map[1].append(code)\n",
    "            permuterm_index_list = permuterm(term)\n",
    "            for perm in permuterm_index_list:\n",
    "                permuterm_index.insert2(perm, i, count)    # insert permutations of term one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheack if 2 soundex codes are near each other or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_soundexes(code1, code2):\n",
    "    result = []\n",
    "    for i in range(4):\n",
    "        if code1[i] == code2[i]:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    if result.count(0) <= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Levenshtein distance between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    # If one of the words is empty, return the length of the other word\n",
    "    if not word1:\n",
    "        return len(word2)\n",
    "    if not word2:\n",
    "        return len(word1)\n",
    "    # If the first characters of the words are the same, ignore them and recurse on the rest of the words\n",
    "    if word1[0] == word2[0]:\n",
    "        return levenshtein_distance(word1[1:], word2[1:])\n",
    "    # Otherwise, consider three possible operations: insertion, deletion, or substitution\n",
    "    # Recurse on each operation and return the minimum cost\n",
    "    insert = 1 + levenshtein_distance(word1, word2[1:])\n",
    "    delete = 1 + levenshtein_distance(word1[1:], word2)\n",
    "    substitute = 1 + levenshtein_distance(word1[1:], word2[1:])\n",
    "    return min(insert, delete, substitute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the misspelled query and find the expected query with levenshtein distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "festival founders "
     ]
    }
   ],
   "source": [
    "def misspelled_query(query):\n",
    "    query.lower()\n",
    "    query_list = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', query)\n",
    "    expected_query = []\n",
    "    for token in query_list:        # check each term in query \n",
    "        distance_check = []\n",
    "        if not permuterm_index.search2(token):   # if the query term is misspelled\n",
    "            code = compute_soundex(token)           # compute its soundex\n",
    "            for i in range(len(soundex_map[1])):      # check all of the soundex map and find the codes near query term's code\n",
    "                code1 = soundex_map[1][i]\n",
    "                result = and_soundexes(code, code1)\n",
    "                if result:\n",
    "                    distance_check.append(soundex_map[0][i])  # add the term to list of checking words\n",
    "        else:\n",
    "            distance_check.append(token)\n",
    "        min_dis = 1e10\n",
    "        spell_res = \"\"\n",
    "        for i in distance_check:       # check all of the near words for levenshtein distance and return the nearest one as a result\n",
    "            dist = levenshtein_distance(i, token)\n",
    "            if dist < min_dis:     # save the min distance and related word\n",
    "                spell_res = i\n",
    "                min_dis = dist\n",
    "\n",
    "        expected_query.append(spell_res)\n",
    "    return expected_query\n",
    "\n",
    "spell_query = input(\"Enter misspelled query: \")\n",
    "expected_query = misspelled_query(spell_query)\n",
    "\n",
    "for w in expected_query:\n",
    "    print(w, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get wildcard query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nobody\n",
      "{0: [237]}\n",
      "nearby\n",
      "{14: [148]}\n",
      "['nobody', 'nearby']\n"
     ]
    }
   ],
   "source": [
    "def wildcard_query(query):\n",
    "    query = query.lower()\n",
    "    query_list = query.split(\"*\")\n",
    "    result = []\n",
    "    if query.count(\"*\") == 2:     # for 2 star wildcards\n",
    "        new_query = query_list[2] + \"$\" + query_list[0] + \"*\"    # turn A*B*C to C$A*\n",
    "        matchs = permuterm_index.find_words(new_query[:-1])   # find all matchs for C$A as a prefix in trie\n",
    "\n",
    "        for i, word in enumerate(matchs):\n",
    "            list_word = word.split(\"$\")\n",
    "            new_word = list_word[1] + list_word[0]\n",
    "            matchs[i] = new_word                        # for all matches turn them to their true format of word\n",
    "\n",
    "        len0 = len(query_list[0])\n",
    "        len2 = len(query_list[2])\n",
    "        matchs2 = []\n",
    "        for word in matchs:\n",
    "            word = word[len0:-len2]       # for all matches delete the parts which match the wildcard\n",
    "            matchs2.append(word)\n",
    "\n",
    "        for i, word in enumerate(matchs2):\n",
    "            if query_list[1] in word:         # if the middle part matches too, find the posting list of the term\n",
    "                print(matchs[i])\n",
    "                result.append(matchs[i])\n",
    "                doc_list = permuterm_index.get2(matchs[i]+\"$\").posting_list\n",
    "                print(doc_list)\n",
    "\n",
    "\n",
    "    if query.count(\"*\") == 1:       # for 1 * wildcard\n",
    "        new_query = query_list[1] + \"$\" + query_list[0] + \"*\"\n",
    "        matchs = permuterm_index.find_words(new_query[:-1])   # find all B$A* for A*B$s\n",
    "\n",
    "        for i, word in enumerate(matchs):\n",
    "            list_word = word.split(\"$\")\n",
    "            new_word = list_word[1] + list_word[0]      # find the real words\n",
    "            matchs[i] = new_word\n",
    "\n",
    "        for i, word in enumerate(matchs):\n",
    "            print(matchs[i])\n",
    "            result.append(matchs[i])\n",
    "            doc_list = permuterm_index.get2(matchs[i]+\"$\").posting_list   # fir all matches get the posting list\n",
    "            print(doc_list)\n",
    "    return result\n",
    "\n",
    "query = input(\"Enter the wildcard query: \")\n",
    "result = wildcard_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n",
      "{11: [191, 201]}\n",
      "founders\n",
      "{0: [268]}\n",
      "The result is: \n",
      "0 A Festival of Books\n",
      "11 Pulling Out Nine Tons of Trash\n",
      "The result is: \n",
      "0 A Festival of Books\n"
     ]
    }
   ],
   "source": [
    "def enhanced_query(query):\n",
    "    query = query.lower()\n",
    "    query_list = query.split(\" \")\n",
    "    result, result1, result2 = [], [], []\n",
    "    if (\"and\" in query_list) | (\"or\" in query_list):\n",
    "        if \"*\" in query_list[0]:\n",
    "            result1 = wildcard_query(query_list[0])\n",
    "        else:\n",
    "            result1 = misspelled_query(query_list[0])\n",
    "        if \"*\" in query_list[2]:\n",
    "            result2 = wildcard_query(query_list[2])\n",
    "        else:\n",
    "            result2 = misspelled_query(query_list[2])\n",
    "            print(result2)\n",
    "\n",
    "        for w1 in result1:\n",
    "            for w2 in result2:\n",
    "                new_query = w1 + \" \" + query_list[1] + \" \" + w2\n",
    "                boolean_query(new_query)\n",
    "\n",
    "    elif \"not\" in query_list:\n",
    "        if \"*\" in query_list[1]:\n",
    "            result = wildcard_query(query_list[1])\n",
    "        else:\n",
    "            result = misspelled_query(query_list[1])\n",
    "        for w1 in result:\n",
    "            new_query = query_list[0] + \" \" + w1\n",
    "            boolean_query(new_query)\n",
    "\n",
    "    elif \"near\" in query_list[1]:\n",
    "        if \"*\" in query_list[0]:\n",
    "            result1 = wildcard_query(query_list[0])\n",
    "        else:\n",
    "            result1 = misspelled_query(query_list[0])\n",
    "        if \"*\" in query_list[2]:\n",
    "            result2 = wildcard_query(query_list[2])\n",
    "        else:\n",
    "            result2 = misspelled_query(query_list[2])\n",
    "\n",
    "        for w1 in result1:\n",
    "            for w2 in result2:\n",
    "                new_query = w1 + \" \" + query_list[1] + \" \" + w2\n",
    "                proximity_query(new_query)\n",
    "\n",
    "query = input(\"Enter enhanced query: \")\n",
    "enhanced_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e26ec1bab8a1abd2e5a712081492a9566af19802245f28e584dda2f9826c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
